1. Merge Sort vs Quick Sort
1b. Bubble Sort vs QuickSort
2. Bubble Sort vs Selection Sort vs Insertion Sort
3. Median of Medians:
4. Bit twiddling Stanford material
4b. Count bits set in a number
4c. Set, Unset, Clear, Toggle a Bit
4d. Absolute Value of a number
4e. Reverse Bits in a number
4f. Find leftmost set bit in a number
4g. Find rightmost set bit in a number
5. String Search:
6. Given 2 sorted arrays of integers, find the nth largest number in sublinear time
7. k'th smallest in Union of Two arrays:
8. K’th Smallest/Largest Element in Unsorted Array | Set 3 (Worst Case Linear Time)
9. GCD of two numbers:
10. STOCK PROBLEMS:
11. Sorted Rotated Array:
12. Find all subsets of a set
13. Multiplication of two numers (without using *)
14. Spiral Level Order traversal of a tree
15. Space Complexity for Tree Traversal.
16. Memoization vs Tabulation in Dynamic Programming
17. How to efficiently check whether it's height balanced for a massively skewed binary search tree
18. How to determine if binary tree is balanced
19. Explain Morris inorder tree traversal without using stacks or recursion
20. Count subsequences divisible by K
21. Merge Overlapping intervals
22. Finding all subsets of a Vector of Vector
23. Converting Recursive Algorithm to Iterative
24. Balancing Binary Search Trees
25. Balancing a Heap
26. Why is Heapify O(n)
26b. SiftUp vs SiftDown
27. n way merge
----------------------------------------------------------------------------------------------------

1.
Merge Sort vs Quick Sort
http://stackoverflow.com/questions/5222730/why-is-merge-sort-preferred-over-quick-sort-for-sorting-linked-lists?rq=1

    "Merge sort is very efficient for immutable datastructures like linked lists"
    "Quick sort is typically faster than merge sort when the data is stored in memory.
     However, when the data set is huge and is stored on external devices such as a hard
     drive, merge sort is the clear winner in terms of speed. It minimizes the expensive
     reads of the external drive"
    "when operating on linked lists, merge sort only requires a small constant amount of
     auxiliary storage"

     Quick Sort: 
     - Best when able to index into an array or similar structure.
       When that's possible, it's hard to beat Quicksort.
     - Quicksort is fast when the data fits into memory and can be addressed directly.

     IMP:
     - Array of equal elements.
       while (a[i] < pivot) {i++;}
       while (pivot < a[j] ) {j--;}
       if (i < j) swap (a[i], a[j]);

       If we have an array of equal elements, the second code will never increment i or decrement j
     http://faculty.simpson.edu/lydia.sinapova/www/cmsc250/LN250_Tremblay/L06-QuickSort.htm

        VERY IMP:
        CHECK IF "i <= j"
        After doing a SWAP, decrement j and increment i.
        This is get over duplicate elements.
        http://www.algolist.net/Algorithms/Sorting/Quicksort

    3 way partition:
        Nice explanation of the algorithm:
        http://algs4.cs.princeton.edu/lectures/23DemoPartitioning.pdf

    Dutch National Flag Problem:
        http://stackoverflow.com/questions/11214089/understanding-dutch-national-flag-program

     Merge Sort:
     - Faster when reading from Disk and not everything can be in memory
     - Mergesort is faster when data won't fit into memory or when it's expensive to get to an item.

1b. Bubble Sort vs QuickSort
    If the array is almost sorted, Bubble sort is faster than Quick Sort

2.
Bubble Sort vs Selection Sort vs Insertion Sort

    http://www.sorting-algorithms.com/selection-sort
    Bubble Sort:
        - Check pair of elements and swap them so that the largest gets bubbled to the end.
        PROS:
            If array is almost sorted OR once we have the array sorted, we can STOP there.

        Bubble sort has many of the same properties as insertion sort, but has slightly higher overhead.
        In the case of nearly sorted data, bubble sort takes O(n) time, but requires at least 2 passes through the data (whereas insertion sort requires something more like 1 pass). 

    Insertion Sort:
        - What we have at any given time is sorted.
        PROS:
            Online Sorting
            Can you binary search for inserting into a position

        Although it is one of the elementary sorting algorithms with O(n2) worst-case time,
        insertion sort is the algorithm of choice either when the data is nearly sorted (because it is adaptive) or when the problem size is small (because it has low overhead).

        For these reasons, and because it is also stable, insertion sort is often used as the recursive base case
        (when the problem size is small) for higher overhead divide-and-conquer sorting algorithms, such as merge sort or quick sort. 


    BUBBLE SORT vs INSERTION SORT
        In insertion sort elements are bubbled into the sorted section, while in bubble sort the maximums are bubbled out of the unsorted section.
        Bubble sort always takes one more pass over array to determine if it's sorted.
        On the other hand, insertion sort not need this -- once last element inserted, algorithm guarantees that array is sorted.

        Bubble sort does n comparisons on every pass.
        Insertion sort does less than n comparisons -- once algorith finds position where to insert current element it stops making comparisons and takes next element.

    Selection Sort:
        Select Minimum element each time and put it at the front.
        PROS:
            - Never have to do MORE THAN "n" swaps.

        From the comparions presented here, one might conclude that selection sort should never be used.
        It does not adapt to the data in any way (notice that the four animations above run in lock step), so its runtime is always quadratic.

        However, selection sort has the property of minimizing the number of swaps.
        In applications where the cost of swapping items is high, selection sort very well may be the algorithm of choice.

3.
Median of Medians:
http://stackoverflow.com/questions/12545795/explanation-of-the-median-of-medians-algorithm
http://stackoverflow.com/questions/9489061/understanding-median-of-medians-algorithm?rq=1

        Think of the following set of numbers: 
        5 2 6 3 1

        The median of these numbers is 3. Now if you have a number n, if n > 3, then it is bigger than at least half of the numbers above. If n < 3, then it is smaller than at least half of the numbers above.

        So that is the idea. That is, for each set of 5 numbers, you get their median. Now you have n / 5 numbers. This is obvious.

        Now if you get the median of those numbers (call it m), it is bigger than half of them and smaller than the other half (by definition of median!). In other words, m is bigger than n / 10 numbers (which themselves were medians of small 5 element groups) and bigger than another n / 10 numbers (which again were medians of small 5 element groups).

        In the example above, we saw that if the median is k and you have m > k, then m is also bigger than 2 other numbers (that were themselves smaller than k). This means that for each of those smaller 5 element groups where m was bigger than its medium, m is bigger also than two other numbers. This makes it at least 3 numbers (2 numbers + the median itself) in each of those n / 10 small 5 element groups, that are smaller than m. Hence, m is at least bigger than 3n/10 numbers.

        Similar logic for the number of elements m is bigger than.


http://stackoverflow.com/questions/9489061/understanding-median-of-medians-algorithm 
        // L is the array on which median of medians needs to be found.
        // k is the expected median position. E.g. first select call might look like:
        // select (array, N/2), where 'array' is an array of numbers of length N

        select(L,k)
        {

            if (L has 5 or fewer elements) {
                sort L
                return the element in the kth position
            }

            partition L into subsets S[i] of five elements each
                (there will be n/5 subsets total).

            for (i = 1 to n/5) do
                x[i] = select(S[i],3)

            M = select({x[i]}, n/10)

            // The code to follow ensures that even if M turns out to be the
            // smallest/largest value in the array, we'll get the kth smallest
            // element in the array

            // Partition array into three groups based on their value as
            // compared to median M

            partition L into L1<M, L2=M, L3>M

            // Compare the expected median position k with length of first array L1
            // Run recursive select over the array L1 if k is less than length
            // of array L1
            if (k <= length(L1))
                return select(L1,k)

            // Check if k falls in L3 array. Recurse accordingly
            else if (k > length(L1)+length(L2))
                return select(L3,k-length(L1)-length(L2))

            // Simply return M since k falls in L2
            else return M
        }

4.
Bit twiddling Stanford material
http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetTable

4b.
Count bits set in a number
http://stackoverflow.com/questions/22081738/how-variable-precision-swar-algorithm-works
http://stackoverflow.com/questions/109023/how-to-count-the-number-of-set-bits-in-a-32-bit-integer

    int SWAR(unsigned int i)
    {
        // Count the number of bits set in TWO bits at a time. Like Bits 1,2; 3,4; 5,6 etc
        i = i - ((i >> 1) & 0x55555555);

        // Count the number of bits set in FOUR bits at a time. Like Bits 1,2,3,4; 5,6,7,8 etc
        i = (i & 0x33333333) + ((i >> 2) & 0x33333333);

        // Count the number of bits set in EIGHT bits 
        // (i + (i >> 4)) & 0x0f0f0f0f
        // since 0x01010101 = (1 << 24) + (1 << 16) + (1 << 8) + 1, we have:
        // k * 0x01010101 = (k << 24) + (k << 16) + (k << 8) + k

        // Add all the number of 1s in EIGHT BITS into the highest ordered byte.
        // We want the last 8 bytes to represent the number of 1s. So right shift >> 24.
        return (((i + (i >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24;
    }

4c.
Set, Unset, Clear, Toggle a Bit
http://stackoverflow.com/questions/47981/how-do-you-set-clear-and-toggle-a-single-bit-in-c-c?rq=1

    Setting a bit
    number |= 1 << x;

    Clearing a bit
    number &= ~(1 << x);

    Toggling a bit
    number ^= 1 << x;

    Checking a bit
    bit = (number >> x) & 1;

4d.
Absolute Value of a number
http://stackoverflow.com/questions/12041632/how-to-compute-the-integer-absolute-value

    Let's assume a twos-complement number (as it's the usual case and you don't say otherwise) and let's assume 32-bit:

    First, we perform an arithmetic right-shift by 31 bits.
    This shifts in all 1s for a negative number or all 0s for a positive one (but note that the actual >>-operator's behaviour in C or C++ is implementation defined for negative numbers, but will usually also perform an arithmetic shift, but let's just assume pseudocode or actual hardware instructions, since it sounds like homework anyway):

        mask = x >> 31;

    So what we get is 111...111 (-1) for negative numbers and 000...000 (0) for positives

    Now we XOR this with x, getting the behaviour of a NOT for mask=111...111 (negative) and a no-op for mask=000...000 (positive):
        x = x XOR mask;

    And finally subtract our mask, which means +1 for negatives and +0/no-op for positives:
        x = x - mask;

    So for positives we perform an XOR with 0 and a subtraction of 0 and thus get the same number.
    And for negatives, we got (NOT x) + 1, which is exactly -x when using twos-complement representation

4e.
Reverse Bits in a number
http://stackoverflow.com/questions/845772/how-to-check-if-the-binary-representation-of-an-integer-is-a-palindrome
http://www.geeksforgeeks.org/write-an-efficient-c-program-to-reverse-bits-of-a-number/

    unsigned m = 0;
    for(unsigned tmp = n; tmp; tmp >>= 1)
    	m = (m << 1) | (tmp & 1);

    unsigned int reverseBits(unsigned int num)
    {
        unsigned int  NO_OF_BITS = sizeof(num) * 8;
        unsigned int reverse_num = 0;
        int i;
        for (i = 0; i < NO_OF_BITS; i++)
        {
            if((num & (1 << i)))
               reverse_num |= 1 << ((NO_OF_BITS - 1) - i);  
       }
        return reverse_num;
    }

    // Using bit shift
        unsigned int v; // 32-bit word to reverse bit order

        // swap odd and even bits
        v = ((v >> 1) & 0x55555555) | ((v & 0x55555555) << 1);
        // swap consecutive pairs
        v = ((v >> 2) & 0x33333333) | ((v & 0x33333333) << 2);
        // swap nibbles ... 
        v = ((v >> 4) & 0x0F0F0F0F) | ((v & 0x0F0F0F0F) << 4);
        // swap bytes
        v = ((v >> 8) & 0x00FF00FF) | ((v & 0x00FF00FF) << 8);
        // swap 2-byte long pairs
        v = ( v >> 16             ) | ( v               << 16);

4f.
Find leftmost set bit in a number
http://stackoverflow.com/questions/53161/find-the-highest-order-bit-in-c

    1 << ( int) log2( x)

    int hibit(unsigned int n)
    {
        n |= (n >>  1);
        n |= (n >>  2);
        n |= (n >>  4);
        n |= (n >>  8);
        n |= (n >> 16);
        return n - (n >> 1);
    }

    int hob (int num)
    {
        if (!num)
            return 0;

        int ret = 1;

        while (num >>= 1)
            ret <<= 1;

        return ret;
    }

    hob(1234) returns 1024
    hob(1024) returns 1024
    hob(1023) returns 512

4g.
Find rightmost set bit in a number
http://algorithmsandme.in/2013/10/fun-with-bits-find-rightmost-bit-set-in-given-number/
http://www.geeksforgeeks.org/position-of-rightmost-set-bit/

    unsigned int getFirstSetBitPos(int n)
    {
       return log2(n&-n)+1;
    }
    
    x & ~(x-1)

To Clear rightmost set bit just do
    x & (x-1)

5.
String Search:
    Bayer More:
    http://www.geeksforgeeks.org/pattern-searching-set-7-boyer-moore-algorithm-bad-character-heuristic/

6.
Given 2 sorted arrays of integers, find the nth largest number in sublinear time
http://stackoverflow.com/questions/4686823/given-2-sorted-arrays-of-integers-find-the-nth-largest-number-in-sublinear-time

    I think this is two concurrent binary searches on the subarrays A[0..n-1] and B[0..n-1], which is O(log n).

        Given sorted arrays, you know that the nth largest will appear somewhere before or at A[n-1] if it is in array A, or B[n-1] if it is in array B
        Consider item at index a in A and item at index b in B.
        Perform binary search as follows (pretty rough pseudocode, not taking in account 'one-off' problems):
            If a + b > n, then reduce the search set
                if A[a] > B[b] then b = b / 2, else a = a / 2
            If a + b < n, then increase the search set
                if A[a] > B[b] then b = 3/2 * b, else a = 3/2 * a (halfway between a and previous a)
            If a + b = n then the nth largest is max(A[a], B[b])

    I believe worst case O(ln n), but in any case definitely sublinear.

7.
k'th smallest in Union of Two arrays:
http://articles.leetcode.com/2011/01/find-k-th-smallest-element-in-union-of.html

8.
K’th Smallest/Largest Element in Unsorted Array | Set 3 (Worst Case Linear Time)
http://www.geeksforgeeks.org/kth-smallestlargest-element-unsorted-array-set-3-worst-case-linear-time/
http://www.programcreek.com/2014/05/leetcode-kth-largest-element-in-an-array-java/

9.
GCD of two numbers:

    GCD(a,b) = GCD(b,a-b)
    GCD(a,b) = GCD(b,a%b)
    NICE IMP Proof:
        https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/the-euclidean-algorithm

10.
STOCK PROBLEMS:

    http://stackoverflow.com/questions/9514191/maximizing-profit-for-given-stock-quotes
    Traverse Backwards

    http://stackoverflow.com/questions/1663545/find-buy-sell-prices-in-array-of-stock-values-to-maximize-positive-difference
    http://stackoverflow.com/questions/7086464/maximum-single-sell-profit


11.
Sorted Rotated Array:

    In a rotated sorted array, only one of A and B can be guaranteed to be sorted. If the element lies within a part which is sorted, then the solution is simple:
        just perform the search as if you were doing a normal binary search. If, however, you must search an unsorted part,
        then just recursively call your search function on the non-sorted part.

    This ends up giving on a time complexity of O(lg n).

12.
Find all subsets of a set
http://stackoverflow.com/questions/728972/finding-all-the-subsets-of-a-set
http://www.geeksforgeeks.org/power-set/
http://stackoverflow.com/questions/15726641/find-all-possible-substring-in-fastest-way

    for (int i = 0; i < A.length(); i++) {
        for (int j = i+1; j <= A.length(); j++) {
            System.out.println(A.substring(i,j));
        }
    }

13.
Multiplication of two numers (without using *)
http://stackoverflow.com/questions/28888068/time-complexity-of-russian-peasant-multiplication-algorithm

    Using LOG:
        10^(log10(A) + log10(B))
        i.e. pow(10, log10(A) + log10(B)

    Use bit shifting AKA 
    https://en.wikipedia.org/wiki/Multiplication_algorithm#Peasant_or_binary_multiplication

    Decimal:     Binary:
    11           3       1011  11
    5            6       101  110
    2            12       10  1100
    1            24       1  11000
                 --         ------
                 33         100001

    The method works because multiplication is distributive, so:

        3 * 11 & = 3 * (1 * 2^0 + 1 * 2^1 + 0 * 2^2 + 1 * 2^3)
                 = 3 * (1 + 2 + 8)
                 = 3 + 6 + 24
                 = 33

    IMP:
    1. Time complexity depends on the NUMBER that we are using for doing the shift
       log (a) or log(b) based on which we select as A
    2. In the above example say 11 = A and 3 = B
       - Select A such that it has lesser number of bits.
       - We are repeating the multiplication till A REACHES 1.
       - So select A as the number that has least number of bits set.

14.
Spiral Level Order traversal of a tree
http://www.geeksforgeeks.org/level-order-traversal-in-spiral-form/
http://stackoverflow.com/questions/17485773/print-level-order-traversal-of-binary-tree-in-zigzag-manner

    You have to use two stacks

        1. first stack for printing from left to right
        2. second stack for printing from right to left.

    Start from the root node.
    Store it's children in one stack.
    In every iteration, you have nodes of one level in one of the stacks.
    Print the nodes, and push nodes of next level in other stack.
    Repeat until your reach the final level.

    Time Complexity O(n) and space complexity O(h).

15.
Space Complexity for Tree Traversal.

    O(h) where h is the height of the tree.

    At any time in the stack we will have ones in that depth level
    So space complexity is height of tree.

    BFS:
    Time complexity is O(|V|) where |V| is the number of nodes,you need to traverse all nodes.
    Space complecity is O(|V|) as well - since at worst case you need to hold all vertices in the queue.

    DFS:
    Time complexity is again O(|V|), you need to traverse all nodes.
    Space complexity - depends on the implementation, a recursive implementation can have a O(h) space complexity [worst case], where h is the maximal depth of your tree.
    Using an iterative solution with a stack is actually the same as BFS, just using a stack instead of a queue - so you get both O(|V|) time and space complexity.
            a
       b         e
     c   d     f   g

    a
    |
    |--> b
    |    |
    |    |--> c
    |    |
    |    |--> d
    |
    |--> e
         |
         |--> f
         |
         |--> g

16.
Memoization vs Tabulation in Dynamic Programming
http://stackoverflow.com/questions/6184869/what-is-difference-between-memoization-and-dynamic-programming

    What is the difference between tabulation (the typical dynamic programming technique) and memoization?

    When you solve a dynamic programming problem using tabulation you solve the problem "bottom up",
    i.e., by solving all related sub-problems first, typically by filling up an n-dimensional table.
    Based on the results in the table, the solution to the "top" / original problem is then computed.

    If you use memoization to solve the problem you do it by maintaining a map of already solved sub problems.
    You do it "top down" in the sense that you solve the "top" problem first (which typically recurses down to solve the sub-problems).

    A good slide from here (link is now dead, slide is still good though):

            If all subproblems must be solved at least once, a bottom-up dynamic-programming algorithm usually outperforms a top-down memoized algorithm by a constant factor
                No overhead for recursion and less overhead for maintaining table
                There are some problems for which the regular pattern of table accesses in the dynamic-programming algorithm can be exploited to reduce the time or space requirements even further
            If some subproblems in the subproblem space need not be solved at all, the memoized solution has the advantage of solving only those subproblems that are definitely required

17.
How to efficiently check whether it's height balanced for a massively skewed binary search tree
http://stackoverflow.com/questions/23160438/how-to-efficiently-check-whether-its-height-balanced-for-a-massively-skewed-bin

18.
How to determine if binary tree is balanced
http://stackoverflow.com/questions/742844/how-to-determine-if-binary-tree-is-balanced?lq=1

19.
Explain Morris inorder tree traversal without using stacks or recursion
http://stackoverflow.com/questions/5502916/explain-morris-inorder-tree-traversal-without-using-stacks-or-recursion

20.
Count subsequences divisible by K
http://stackoverflow.com/questions/24518682/count-subsequences-divisible-by-k
https://www.hackerrank.com/contests/w6/challenges/consecutive-subsequences/editorial

21.
Merge Overlapping intervals
http://www.geeksforgeeks.org/merging-intervals/
http://stackoverflow.com/questions/4542892/possible-interview-question-how-to-find-all-overlapping-intervals
http://stackoverflow.com/questions/15150188/amazon-interview-design-meeting-scheduler

22.
Finding all subsets of a Vector of Vector
http://stackoverflow.com/questions/728972/finding-all-the-subsets-of-a-set

    It's very simple to do this recursively.
    The basic idea is that for each element, the set of subsets can be divided equally into those that contain that element and those that don't, and those two sets are otherwise equal.
        For n=1, the set of subsets is {{}, {1}}
        For n>1, find the set of subsets of 1,...,n-1 and make two copies of it. For one of them, add n to each subset. Then take the union of the two copies.
    Edit To make it crystal clear:
        The set of subsets of {1} is {{}, {1}}
        For {1, 2}, take {{}, {1}}, add 2 to each subset to get {{2}, {1, 2}} and take the union with {{}, {1}} to get {{}, {1}, {2}, {1, 2}}
    Repeat till you reach n

23.
Converting Recursive Algorithm to Iterative
http://stackoverflow.com/questions/159590/way-to-go-from-recursion-to-iteration
http://programmers.stackexchange.com/questions/194646/what-methods-are-there-to-avoid-a-stack-overflow-in-a-recursive-algorithm

    Usually, I replace a recursive algorithm by an iterative algorithm by pushing the parameters that would normally be passed to the recursive function onto a stack.
    In fact, you are replacing the program stack by one of your own.

        Stack<Object> stack;
        stack.push(first_object);
        while( !stack.isEmpty() ) {
           // Do something
           my_object = stack.pop();

          // Push other objects on the stack.

        }

    Note: if you have more than one recursive call inside and you want to preserve the order of the calls, you have to add them in the reverse order to the stack:

        foo(first);
        foo(second);

        has to be replaced by

        stack.push(second);
        stack.push(first);

24.
Balancing Binary Search Trees on a whole
http://stackoverflow.com/questions/14001676/balancing-a-bst

    Day-Stout-Warren:
    1. Using tree rotations, convert the tree into a degenerate linked list.
    2. By applying selective rotations to the linked list, convert the list back into a completely balanced tree.


    The solution is simple - build an "empty" complete binary tree, and iterate the new tree and the input tree (simultaneously) in inorder-traversal to fill the complete tree.
    When you are done, you have the most balanced tree you can get, and time complexity of this approach is O(n)

24b.
Balancing Binary Search Trees after each insertion
How AVL trees gets self balanced
http://www.geeksforgeeks.org/avl-tree-set-1-insertion/

    Left Rotation and Right Rotation
        Following are two basic operations that can be performed to re-balance a BST without violating the BST property (keys(left) < key(root) < keys(right)).
        1) Left Rotation
        2) Right Rotation

        T1, T2 and T3 are subtrees of the tree rooted with y (on left side) 
        or x (on right side)           
                        y                               x
                       / \     Right Rotation          /  \
                      x   T3   – – – – – – – >        T1   y 
                     / \       < - - - - - - -            / \
                    T1  T2     Left Rotation            T2  T3
        Keys in both of the above trees follow the following order 
              keys(T1) < key(x) < keys(T2) < key(y) < keys(T3)
        So BST property is not violated anywhere.

24c.
B+ Tree vsRed Black Trees vs AVL trees
http://stackoverflow.com/questions/13852870/red-black-tree-over-avl-tree
http://stackoverflow.com/questions/1589556/when-to-choose-rb-tree-b-tree-or-avl-tree

    - B-tree when you're managing more than thousands of items and you're paging them from a disk or some slow storage medium.
    - RB tree when you're doing fairly frequent inserts, deletes and retrievals on the tree.
    - AVL tree when your inserts and deletes are infrequent relative to your retrievals.

    1. What's the main aim we are chossing Red black tree instead of Avl tree?
        Both red-black trees and AVL trees are the most commonly used balanced binary search trees and they support insertion, deletion and look-up in guaranteed O(logN) time.
        However, there are following points of comparison between the two:

            1. AVL trees are more rigidly balanced and hence provide faster look-ups. Thus for a look-up intensive task use an AVL tree.
            2. For an insert intensive taks, use a Red-Black tree.
            3. AVL trees store the balance factor at each node.
               This takes O(N) extra space.
               However, if we know that the keys that will be inserted in the tree will always be greater than zero, we can use the sign bit of the keys to store the colour information of a red-black tree.
               Thus, in such cases red-black tree takes O(1) extra space.
            4. In general, the rotations for an AVL tree are harder to implement and debug than that for a Red-Black tree.

http://stackoverflow.com/questions/16257761/difference-between-red-black-trees-and-avl-trees
        IMP:
        - AVL trees maintain a more rigid balance than red-black trees.
        - The path from the root to the deepest leaf in an AVL tree is at most ~1.44 lg(n+2), while in red black trees it's at most ~2 lg (n+1).

        - As a result, lookup in an AVL tree is typically faster, but this comes at the cost of slower insertion and deletion due to more rotation operations.
        - So use an AVL tree if you expect the number of lookups to dominate the number of updates to the tree.

    2. What are the application of Red black tree?
        Red-black trees are more general purpose.
        They do relatively well on add, remove, and look-up but AVL trees have faster look-ups at the cost of slower add/remove.
        Red-black tree is used in the following:
            - Java: java.util.TreeMap , java.util.TreeSet .
            - C++ STL: map, multimap, multiset.
            - Linux kernel: completely fair scheduler, linux/rbtree.h

    LARGE DATA vs SMALL DATA: AVL vs RB Tree:
        For small data:
        1. insert:
           RB tree & avl tree has constant number of max rotation but RB tree will be faster because on average RB tree use less rotation.
        2. lookup:
           AVL tree is faster, because AVL tree has less depth.
        3. delete:
           RB tree has constant number of max rotation but AVL tree can have O(log N) times of rotation as worst. and on average RB tree also has less number of rotation thus RB tree is faster.

        for large data:
        1. insert:
           AVL tree is faster. because you need to lookup for a particular node before insertion. as you have more data the time difference on looking up the particular node grows proportional to O(log N). but AVL tree & RB tree still only need constant number of rotation at the worst case. Thus the bottle neck will become the time you lookup for that particular node.
        2. lookup:
           AVL tree is faster. (same as in small data case)
        3. delete:
           AVL tree is faster on average, but in worst case RB tree is faster. because you also need to lookup for a very deep node to swap before removal (similar to the reason of insertion). on average both trees has constant number of rotation. but RB tree has a constant upper bound for rotation.

25.
Balancing a Heap


26.
Why is Heapify O(n)
http://stackoverflow.com/questions/9755721/how-can-building-a-heap-be-on-time-complexity

    When heapify is called, the running time depends on how far an element might move down in tree before the process terminates.
    In other words, it depends on the height of the element in the heap.
    In the worst case, the element might go down all the way to the leaf level.

    Let us count the work done level by level.
    At the bottommost level, there are 2^(h)nodes, but we do not call heapify on any of these, so the work is 0.
    At the next to level there are 2^(h - 1) nodes, and each might move down by 1 level.
    At the 3rd level from the bottom, there are 2^(h - 2) nodes, and each might move down by 2 levels.

    As you can see not all heapify operations are O(log n), this is why you are getting O(n)

26b.
SiftUp vs SiftDown
https://en.wikipedia.org/wiki/Heapsort
http://stackoverflow.com/questions/9755721/how-can-building-a-heap-be-on-time-complexity
https://www.youtube.com/watch?v=MiyLo8adrWw&feature=youtu.be

    Heapify is O(n) when done with siftDown but O(n log n) when done with siftUp.
    The actual sorting (pulling items from heap one by one) has to be done with siftUp so is therefore O(n log n)

    - siftDown swaps a node that is too small with its largest child (thereby moving it down) until it is at least as large as both nodes below it.
    - siftUp swaps a node that is too large with its parent (thereby moving it up) until it is no larger than the node above it.

    The heapify procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property.
    An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand.
    This siftUp version can be visualized as starting with an empty heap and successively inserting elements, whereas the siftDown version given above treats the entire input array as a full but "broken" heap and "repairs" it starting from the last non-trivial sub-heap (that is, the last parent node).

    Also, the siftDown version of heapify has O(n) time complexity, while the siftUp version given below has O(n log n) time complexity due to its equivalence with inserting each element, one at a time, into an empty heap

    SIFTUP:
        - Insertion of a new element into a heap is done in two steps. 
        - First the element is inserted in the only place with room for it,- At the bottom of the tree, - And next it is shifted up the tree until the heap property holds. 
        - The insertion is trivial, but the siftup deserves extra attention because of its importance for the performance. 

    SIFTDOWN:
        - Deletion of the maximum element from the heap must find an element to replace the maximum element (in the root).
        - As for insertion this is done in to steps.
        - First the maximum element (the root) is replaced with the last element of the heap, and next the new root is shifted down the tree along the path of the largest children.
    
    The number of operations required for each operation is proportional to the distance the node may have to move.
    For siftDown, it is the distance from the bottom of the tree, so siftDown is expensive for nodes at the top of the tree.
    With siftUp, the work is proportional to the distance from the top of the tree, so siftUp is expensive for nodes at the bottom of the tree.
    Although both operations are O(log n) in the worst case, in a heap, only one node is at the top whereas half the nodes lie in the bottom layer.
    So it shouldn't be too surprising that if we have to apply an operation to every node, we would prefer siftDown over siftUp.

    The buildHeap function takes an array of unsorted items and moves them until it they all satisfy the heap property.
    There are two approaches one might take for buildHeap.
    One is to start at the top of the heap (the beginning of the array) and call siftUp on each item.
    At each step, the previously sifted items (the items before the current item in the array) form a valid heap, and sifting the next item up places it into a valid position in the heap.
    After sifting up each node, all items satisfy the heap property.
    The second approach goes in the opposite direction: start at the end of the array and move backwards towards the front. At each iteration, you sift an item down until it is in the correct location.


    Both of these solutions will produce a valid heap. The question is: which implementation for buildHeap is more efficient? Unsurprisingly, it is the second operation that uses siftDown. If h = log n is the height, then the work required for the siftDown approach is given by the sum

    (0 * n/2) + (1 * n/4) + (2 * n/8) + ... + (h * 1).
    i.e. n/2 nodes might have to move 0 level
         n/4 nodes might have to move 1 level
         n/8 nodes might have to move 2 level
         ...

    Each term in the sum has the maximum distance a node at the given height will have to move (zero for the bottom layer, h for the root) multiplied by the number of nodes at that height. In contrast, the sum for calling siftUp on each node is

    (h * n/2) + ((h-1) * n/4) + ((h-2)*n/8) + ... + (0 * 1).
    i.e. n/2 nodes might have to move h level

    It should be clear that the second sum is larger. The first term alone is hn/2 = 1/2 n log n, so this approach has complexity at best O(n log n). However, the sum for the siftDown approach can be bounded by extending it to a Taylor series to show that it is indeed O(n). If there is interest, I can edit my answer to include the details. Obviously, O(n) is the best you could hope for.

27.
n way merge


