1. Merge Sort vs Quick Sort
2. Bubble Sort vs Selection Sort vs Insertion Sort
3. Median of Medians:
4. Bit twiddling Stanford material
4b. Count bits set in a number
4c. Set, Unset, Clear, Toggle a Bit
4d. Absolute Value of a number
4e. Reverse Bits in a number
5. String Search:
6. Given 2 sorted arrays of integers, find the nth largest number in sublinear time
7. k'th smallest in Union of Two arrays:
8. K’th Smallest/Largest Element in Unsorted Array | Set 3 (Worst Case Linear Time)
9. GCD of two numbers:
10. STOCK PROBLEMS:
11. Sorted Rotated Array:
12. Find all subsets of a set
13. Multiplication of two numers
14. Spiral Level Order traversal of a tree
15. Space Complexity for Tree Traversal.
16. Memoization vs Tabulation in Dynamic Programming
17. How to efficiently check whether it's height balanced for a massively skewed binary search tree
18. How to determine if binary tree is balanced
19. Explain Morris inorder tree traversal without using stacks or recursion
20. Count subsequences divisible by K
21. Merge Overlapping intervals
22. Finding all subsets of a Vector of Vector
23. Converting Recursive Algorithm to Iterative

1.
Merge Sort vs Quick Sort
http://stackoverflow.com/questions/5222730/why-is-merge-sort-preferred-over-quick-sort-for-sorting-linked-lists?rq=1

    "Merge sort is very efficient for immutable datastructures like linked lists"
    "Quick sort is typically faster than merge sort when the data is stored in memory.
     However, when the data set is huge and is stored on external devices such as a hard
     drive, merge sort is the clear winner in terms of speed. It minimizes the expensive
     reads of the external drive"
    "when operating on linked lists, merge sort only requires a small constant amount of
     auxiliary storage"

     Quick Sort: 
     - Best when able to index into an array or similar structure.
       When that's possible, it's hard to beat Quicksort.
     - Quicksort is fast when the data fits into memory and can be addressed directly.

     IMP:
     - Array of equal elements.
       while (a[i] < pivot) {i++;}
       while (pivot < a[j] ) {j--;}
       if (i < j) swap (a[i], a[j]);

       If we have an array of equal elements, the second code will never increment i or decrement j
     http://faculty.simpson.edu/lydia.sinapova/www/cmsc250/LN250_Tremblay/L06-QuickSort.htm

        VERY IMP:
        CHECK IF "i <= j"
        After doing a SWAP, decrement j and increment i.
        This is get over duplicate elements.
        http://www.algolist.net/Algorithms/Sorting/Quicksort

    3 way partition:
        Nice explanation of the algorithm:
        http://algs4.cs.princeton.edu/lectures/23DemoPartitioning.pdf

    Dutch National Flag Problem:
        http://stackoverflow.com/questions/11214089/understanding-dutch-national-flag-program

     Merge Sort:
     - Faster when reading from Disk and not everything can be in memory
     - Mergesort is faster when data won't fit into memory or when it's expensive to get to an item.

2.
Bubble Sort vs Selection Sort vs Insertion Sort

    http://www.sorting-algorithms.com/selection-sort
    Bubble Sort:
        - Check pair of elements and swap them so that the largest gets bubbled to the end.
        PROS:
            If array is almost sorted OR once we have the array sorted, we can STOP there.

        Bubble sort has many of the same properties as insertion sort, but has slightly higher overhead.
        In the case of nearly sorted data, bubble sort takes O(n) time, but requires at least 2 passes through the data (whereas insertion sort requires something more like 1 pass). 

    Insertion Sort:
        - What we have at any given time is sorted.
        PROS:
            Online Sorting
            Can you binary search for inserting into a position

        Although it is one of the elementary sorting algorithms with O(n2) worst-case time,
        insertion sort is the algorithm of choice either when the data is nearly sorted (because it is adaptive) or when the problem size is small (because it has low overhead).

        For these reasons, and because it is also stable, insertion sort is often used as the recursive base case
        (when the problem size is small) for higher overhead divide-and-conquer sorting algorithms, such as merge sort or quick sort. 


    BUBBLE SORT vs INSERTION SORT
        Bubble sort always takes one more pass over array to determine if it's sorted.
        On the other hand, insertion sort not need this -- once last element inserted, algorithm guarantees that array is sorted.

        Bubble sort does n comparisons on every pass.
        Insertion sort does less than n comparisons -- once algorith finds position where to insert current element it stops making comparisons and takes next element.

    Selection Sort:
        Select Minimum element each time and put it at the front.
        PROS:
            - Never have to do MORE THAN "n" swaps.

        From the comparions presented here, one might conclude that selection sort should never be used.
        It does not adapt to the data in any way (notice that the four animations above run in lock step), so its runtime is always quadratic.

        However, selection sort has the property of minimizing the number of swaps.
        In applications where the cost of swapping items is high, selection sort very well may be the algorithm of choice.

3.
Median of Medians:
http://stackoverflow.com/questions/12545795/explanation-of-the-median-of-medians-algorithm
http://stackoverflow.com/questions/9489061/understanding-median-of-medians-algorithm?rq=1

        Think of the following set of numbers: 
        5 2 6 3 1

        The median of these numbers is 3. Now if you have a number n, if n > 3, then it is bigger than at least half of the numbers above. If n < 3, then it is smaller than at least half of the numbers above.

        So that is the idea. That is, for each set of 5 numbers, you get their median. Now you have n / 5 numbers. This is obvious.

        Now if you get the median of those numbers (call it m), it is bigger than half of them and smaller than the other half (by definition of median!). In other words, m is bigger than n / 10 numbers (which themselves were medians of small 5 element groups) and bigger than another n / 10 numbers (which again were medians of small 5 element groups).

        In the example above, we saw that if the median is k and you have m > k, then m is also bigger than 2 other numbers (that were themselves smaller than k). This means that for each of those smaller 5 element groups where m was bigger than its medium, m is bigger also than two other numbers. This makes it at least 3 numbers (2 numbers + the median itself) in each of those n / 10 small 5 element groups, that are smaller than m. Hence, m is at least bigger than 3n/10 numbers.

        Similar logic for the number of elements m is bigger than.


http://stackoverflow.com/questions/9489061/understanding-median-of-medians-algorithm 
        // L is the array on which median of medians needs to be found.
        // k is the expected median position. E.g. first select call might look like:
        // select (array, N/2), where 'array' is an array of numbers of length N

        select(L,k)
        {

            if (L has 5 or fewer elements) {
                sort L
                return the element in the kth position
            }

            partition L into subsets S[i] of five elements each
                (there will be n/5 subsets total).

            for (i = 1 to n/5) do
                x[i] = select(S[i],3)

            M = select({x[i]}, n/10)

            // The code to follow ensures that even if M turns out to be the
            // smallest/largest value in the array, we'll get the kth smallest
            // element in the array

            // Partition array into three groups based on their value as
            // compared to median M

            partition L into L1<M, L2=M, L3>M

            // Compare the expected median position k with length of first array L1
            // Run recursive select over the array L1 if k is less than length
            // of array L1
            if (k <= length(L1))
                return select(L1,k)

            // Check if k falls in L3 array. Recurse accordingly
            else if (k > length(L1)+length(L2))
                return select(L3,k-length(L1)-length(L2))

            // Simply return M since k falls in L2
            else return M
        }

4.
Bit twiddling Stanford material
http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetTable

4b.
Count bits set in a number
http://stackoverflow.com/questions/22081738/how-variable-precision-swar-algorithm-works
http://stackoverflow.com/questions/109023/how-to-count-the-number-of-set-bits-in-a-32-bit-integer

    int SWAR(unsigned int i)
    {
        // Count the number of bits set in TWO bits at a time. Like Bits 1,2; 3,4; 5,6 etc
        i = i - ((i >> 1) & 0x55555555);

        // Count the number of bits set in FOUR bits at a time. Like Bits 1,2,3,4; 5,6,7,8 etc
        i = (i & 0x33333333) + ((i >> 2) & 0x33333333);

        // Count the number of bits set in EIGHT bits 
        // (i + (i >> 4)) & 0x0f0f0f0f
        // since 0x01010101 = (1 << 24) + (1 << 16) + (1 << 8) + 1, we have:
        // k * 0x01010101 = (k << 24) + (k << 16) + (k << 8) + k

        // Add all the number of 1s in EIGHT BITS into the highest ordered byte.
        // We want the last 8 bytes to represent the number of 1s. So right shift >> 24.
        return (((i + (i >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24;
    }

4c.
Set, Unset, Clear, Toggle a Bit
http://stackoverflow.com/questions/47981/how-do-you-set-clear-and-toggle-a-single-bit-in-c-c?rq=1

    Setting a bit
    number |= 1 << x;

    Clearing a bit
    number &= ~(1 << x);

    Toggling a bit
    number ^= 1 << x;

    Checking a bit
    bit = (number >> x) & 1;

4d.
Absolute Value of a number
http://stackoverflow.com/questions/12041632/how-to-compute-the-integer-absolute-value

    Let's assume a twos-complement number (as it's the usual case and you don't say otherwise) and let's assume 32-bit:

    First, we perform an arithmetic right-shift by 31 bits.
    This shifts in all 1s for a negative number or all 0s for a positive one (but note that the actual >>-operator's behaviour in C or C++ is implementation defined for negative numbers, but will usually also perform an arithmetic shift, but let's just assume pseudocode or actual hardware instructions, since it sounds like homework anyway):

        mask = x >> 31;

    So what we get is 111...111 (-1) for negative numbers and 000...000 (0) for positives

    Now we XOR this with x, getting the behaviour of a NOT for mask=111...111 (negative) and a no-op for mask=000...000 (positive):
        x = x XOR mask;

    And finally subtract our mask, which means +1 for negatives and +0/no-op for positives:
        x = x - mask;

    So for positives we perform an XOR with 0 and a subtraction of 0 and thus get the same number.
    And for negatives, we got (NOT x) + 1, which is exactly -x when using twos-complement representation

4e.
Reverse Bits in a number
http://stackoverflow.com/questions/845772/how-to-check-if-the-binary-representation-of-an-integer-is-a-palindrome
http://www.geeksforgeeks.org/write-an-efficient-c-program-to-reverse-bits-of-a-number/

    for(unsigned tmp = n; tmp; tmp >>= 1)
    	m = (m << 1) | (tmp & 1);

    unsigned int reverseBits(unsigned int num)
    {
        unsigned int  NO_OF_BITS = sizeof(num) * 8;
        unsigned int reverse_num = 0;
        int i;
        for (i = 0; i < NO_OF_BITS; i++)
        {
            if((num & (1 << i)))
               reverse_num |= 1 << ((NO_OF_BITS - 1) - i);  
       }
        return reverse_num;
    }

    // Using bit shift
        unsigned int v; // 32-bit word to reverse bit order

        // swap odd and even bits
        v = ((v >> 1) & 0x55555555) | ((v & 0x55555555) << 1);
        // swap consecutive pairs
        v = ((v >> 2) & 0x33333333) | ((v & 0x33333333) << 2);
        // swap nibbles ... 
        v = ((v >> 4) & 0x0F0F0F0F) | ((v & 0x0F0F0F0F) << 4);
        // swap bytes
        v = ((v >> 8) & 0x00FF00FF) | ((v & 0x00FF00FF) << 8);
        // swap 2-byte long pairs
        v = ( v >> 16             ) | ( v               << 16);

5.
String Search:

    Bayer More:
    http://www.geeksforgeeks.org/pattern-searching-set-7-boyer-moore-algorithm-bad-character-heuristic/

6.
Given 2 sorted arrays of integers, find the nth largest number in sublinear time
http://stackoverflow.com/questions/4686823/given-2-sorted-arrays-of-integers-find-the-nth-largest-number-in-sublinear-time

    I think this is two concurrent binary searches on the subarrays A[0..n-1] and B[0..n-1], which is O(log n).

        Given sorted arrays, you know that the nth largest will appear somewhere before or at A[n-1] if it is in array A, or B[n-1] if it is in array B
        Consider item at index a in A and item at index b in B.
        Perform binary search as follows (pretty rough pseudocode, not taking in account 'one-off' problems):
            If a + b > n, then reduce the search set
                if A[a] > B[b] then b = b / 2, else a = a / 2
            If a + b < n, then increase the search set
                if A[a] > B[b] then b = 3/2 * b, else a = 3/2 * a (halfway between a and previous a)
            If a + b = n then the nth largest is max(A[a], B[b])

    I believe worst case O(ln n), but in any case definitely sublinear.

7.
k'th smallest in Union of Two arrays:
http://articles.leetcode.com/2011/01/find-k-th-smallest-element-in-union-of.html

8.
K’th Smallest/Largest Element in Unsorted Array | Set 3 (Worst Case Linear Time)
http://www.geeksforgeeks.org/kth-smallestlargest-element-unsorted-array-set-3-worst-case-linear-time/
http://www.programcreek.com/2014/05/leetcode-kth-largest-element-in-an-array-java/

9.
GCD of two numbers:

    GCD(a,b) = GCD(b,a-b)
    GCD(a,b) = GCD(b,a%b)
    NICE IMP Proof:
        https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/the-euclidean-algorithm

10.
STOCK PROBLEMS:

    http://stackoverflow.com/questions/9514191/maximizing-profit-for-given-stock-quotes
    Traverse Backwards

    http://stackoverflow.com/questions/1663545/find-buy-sell-prices-in-array-of-stock-values-to-maximize-positive-difference
    http://stackoverflow.com/questions/7086464/maximum-single-sell-profit


11.
Sorted Rotated Array:

    In a rotated sorted array, only one of A and B can be guaranteed to be sorted. If the element lies within a part which is sorted, then the solution is simple:
        just perform the search as if you were doing a normal binary search. If, however, you must search an unsorted part,
        then just recursively call your search function on the non-sorted part.

    This ends up giving on a time complexity of O(lg n).

12.
Find all subsets of a set
http://stackoverflow.com/questions/728972/finding-all-the-subsets-of-a-set
http://www.geeksforgeeks.org/power-set/
http://stackoverflow.com/questions/15726641/find-all-possible-substring-in-fastest-way

    for (int i = 0; i < A.length(); i++) {
        for (int j = i+1; j <= A.length(); j++) {
            System.out.println(A.substring(i,j));
        }
    }

13.
Multiplication of two numers
http://stackoverflow.com/questions/28888068/time-complexity-of-russian-peasant-multiplication-algorithm

    Use bit shifting AKA 

    https://en.wikipedia.org/wiki/Multiplication_algorithm#Peasant_or_binary_multiplication

    Decimal:     Binary:
    11           3       1011  11
    5            6       101  110
    2            12       10  1100
    1            24       1  11000
                 --         ------
                 33         100001

    The method works because multiplication is distributive, so:

        3 * 11 & = 3 * (1 * 2^0 + 1 * 2^1 + 0 * 2^2 + 1 * 2^3)
                 = 3 * (1 + 2 + 8)
                 = 3 + 6 + 24
                 = 33

    IMP:
    1. Time complexity depends on the NUMBER that we are using for doing the shift
       log (a) or log(b) based on which we select as A
    2. In the above example say 11 = A and 3 = B
       - Select A such that it has lesser number of bits.
       - We are repeating the multiplication till A REACHES 1.
       - So select A as the number that has least number of bits set.

14.
Spiral Level Order traversal of a tree
http://www.geeksforgeeks.org/level-order-traversal-in-spiral-form/
http://stackoverflow.com/questions/17485773/print-level-order-traversal-of-binary-tree-in-zigzag-manner

    You have to use two stacks

        1. first stack for printing from left to right
        2. second stack for printing from right to left.

    Start from the root node.
    Store it's children in one stack.
    In every iteration, you have nodes of one level in one of the stacks.
    Print the nodes, and push nodes of next level in other stack.
    Repeat until your reach the final level.

    Time Complexity O(n) and space complexity O(h).

15.
Space Complexity for Tree Traversal.

    O(h) where h is the height of the tree.

    At any time in the stack we will have ones in that depth level
    So space complexity is height of tree.

    BFS:
    Time complexity is O(|V|) where |V| is the number of nodes,you need to traverse all nodes.
    Space complecity is O(|V|) as well - since at worst case you need to hold all vertices in the queue.

    DFS:
    Time complexity is again O(|V|), you need to traverse all nodes.
    Space complexity - depends on the implementation, a recursive implementation can have a O(h) space complexity [worst case], where h is the maximal depth of your tree.
    Using an iterative solution with a stack is actually the same as BFS, just using a stack instead of a queue - so you get both O(|V|) time and space complexity.
            a
       b         e
     c   d     f   g

    a
    |
    |--> b
    |    |
    |    |--> c
    |    |
    |    |--> d
    |
    |--> e
         |
         |--> f
         |
         |--> g

16.
Memoization vs Tabulation in Dynamic Programming
http://stackoverflow.com/questions/6184869/what-is-difference-between-memoization-and-dynamic-programming

    What is the difference between tabulation (the typical dynamic programming technique) and memoization?

    When you solve a dynamic programming problem using tabulation you solve the problem "bottom up",
    i.e., by solving all related sub-problems first, typically by filling up an n-dimensional table.
    Based on the results in the table, the solution to the "top" / original problem is then computed.

    If you use memoization to solve the problem you do it by maintaining a map of already solved sub problems.
    You do it "top down" in the sense that you solve the "top" problem first (which typically recurses down to solve the sub-problems).

    A good slide from here (link is now dead, slide is still good though):

            If all subproblems must be solved at least once, a bottom-up dynamic-programming algorithm usually outperforms a top-down memoized algorithm by a constant factor
                No overhead for recursion and less overhead for maintaining table
                There are some problems for which the regular pattern of table accesses in the dynamic-programming algorithm can be exploited to reduce the time or space requirements even further
            If some subproblems in the subproblem space need not be solved at all, the memoized solution has the advantage of solving only those subproblems that are definitely required

17.
How to efficiently check whether it's height balanced for a massively skewed binary search tree
http://stackoverflow.com/questions/23160438/how-to-efficiently-check-whether-its-height-balanced-for-a-massively-skewed-bin

18.
How to determine if binary tree is balanced
http://stackoverflow.com/questions/742844/how-to-determine-if-binary-tree-is-balanced?lq=1

19.
Explain Morris inorder tree traversal without using stacks or recursion
http://stackoverflow.com/questions/5502916/explain-morris-inorder-tree-traversal-without-using-stacks-or-recursion

20.
Count subsequences divisible by K
http://stackoverflow.com/questions/24518682/count-subsequences-divisible-by-k
https://www.hackerrank.com/contests/w6/challenges/consecutive-subsequences/editorial

21.
Merge Overlapping intervals
http://www.geeksforgeeks.org/merging-intervals/
http://stackoverflow.com/questions/4542892/possible-interview-question-how-to-find-all-overlapping-intervals
http://stackoverflow.com/questions/15150188/amazon-interview-design-meeting-scheduler

22.
Finding all subsets of a Vector of Vector
http://stackoverflow.com/questions/728972/finding-all-the-subsets-of-a-set

    It's very simple to do this recursively.
    The basic idea is that for each element, the set of subsets can be divided equally into those that contain that element and those that don't, and those two sets are otherwise equal.
        For n=1, the set of subsets is {{}, {1}}
        For n>1, find the set of subsets of 1,...,n-1 and make two copies of it. For one of them, add n to each subset. Then take the union of the two copies.
    Edit To make it crystal clear:
        The set of subsets of {1} is {{}, {1}}
        For {1, 2}, take {{}, {1}}, add 2 to each subset to get {{2}, {1, 2}} and take the union with {{}, {1}} to get {{}, {1}, {2}, {1, 2}}
    Repeat till you reach n

23.
Converting Recursive Algorithm to Iterative
http://stackoverflow.com/questions/159590/way-to-go-from-recursion-to-iteration
http://programmers.stackexchange.com/questions/194646/what-methods-are-there-to-avoid-a-stack-overflow-in-a-recursive-algorithm

    Usually, I replace a recursive algorithm by an iterative algorithm by pushing the parameters that would normally be passed to the recursive function onto a stack.
    In fact, you are replacing the program stack by one of your own.

        Stack<Object> stack;
        stack.push(first_object);
        while( !stack.isEmpty() ) {
           // Do something
           my_object = stack.pop();

          // Push other objects on the stack.

        }

    Note: if you have more than one recursive call inside and you want to preserve the order of the calls, you have to add them in the reverse order to the stack:

        foo(first);
        foo(second);

        has to be replaced by

        stack.push(second);
        stack.push(first);

